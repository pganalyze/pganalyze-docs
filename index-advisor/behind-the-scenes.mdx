---
title: 'Index Advisor: Behind the Scenes'
backlink_href: /docs/index-advisor
backlink_title: 'Index Advisor (in-app in pganalyze)'
---

## How Index Advisor detects missing index opportunities

When the Postgres query planner decides how to execute a query, it considers a
number of factors to determine if existing indexes can help the query complete
more efficiently. The Index Advisor builds on this core Postgres code and
extends it to determine what indexes could be useful even if they do not exist.

By evaluating many different possible plans, the Index Advisor can find index
recommendations to help you optimize your schema for your specific queries.
Since pganalyze already monitors information about your schema (like table and
index definitions and sizes), we can use this to analyze any of your queries,
without requiring specific EXPLAIN plan samples.


## The Index Advisor model

The Index Advisor model used for recommendations is built on:

 - Scans, extracted from the full query workload (collected by means of
   pg_stat_statements)
 - I/O estimates through the use of costing logic aligning with the Postgres
   cost model (similar cost to the one shown in an EXPLAIN plan)
 - Prioritizing indexes for frequent queries over infrequent queries
 - Optimizing for reducing the amount of index pages fetched by prioritizing
   conditions with higher index selectivity (high selectivity = match a small
   number of rows)

Future improvements to the Index Advisor may refine this model further. Index
Advisor currently runs on a daily schedule, and looks at data in 7
day-intervals.


## Scans

What exactly are scans? Let’s look at an example, in a simple query like this:

```sql
SELECT * FROM users
  JOIN comments ON (users.id = comments.user_id)
 WHERE users.site_id = 123 AND comments.parent_id IS NULL
 ORDER BY comments.created_at DESC
 LIMIT 10;
```

This query has two scans:

- users:
  - WHERE clause: (site_id = $n)
  - JOIN clause: (id = $n)
- comments:
  - WHERE clause: (parent_id IS NULL)
  - JOIN clause: (user_id = $n)
  - ORDER BY clause: created_at

Each of these scans also has an associated average estimated cost, that is
averaged across all queries it’s referenced in. Note we replace specific values
and fields from other tables with the $n parameter reference for easier
grouping. 

More broadly, each scan consists of:

 - Zero or more WHERE clause conditions
 - Zero or more JOIN clause conditions
 - Zero or more an ORDER BY clause columns (Note: not yet considered by Index
   Advisor; see [Limitations](/docs/index-advisor/limitations))
 - Optionally, a LIMIT on how much data needs to be returned (Note: not yet
   considered by Index Advisor; see
   [Limitations](/docs/index-advisor/limitations))

In case of JOIN conditions, a scan is considered twice, once as a plain index
scan (without JOIN conditions), and once as a [parameterized index
scan](https://pganalyze.com/blog/how-postgres-chooses-index#parameterized-index-scans-or-why-nested-loop-are-sometimes-a-good-join-type)
(with JOIN conditions, which are only usable to target an index when the table
is the inner relation in a Nested Loop). When displayed, the costs of these two
types of index scans are averaged out to show one number.


## The Index Advisor Cost Model

When cost is shown it represents the total cost of performing the scan (i.e. to
retrieve all rows requested) with the currently available scan method
(Sequential Scan or Index Scan). This is the same notion of “cost” and “total
cost” you see when running an EXPLAIN (without ANALYZE) in Postgres. The “cost”
units are a unique measurement used for modeling only, and do not represent
time, or bytes.

The biggest contributor to the estimated cost for a Sequential Scan or Index
Scan is the estimated I/O for the scan. This is based on the Postgres cost
estimation functions used by the Postgres planner. You can read more about this
in [our recent blog
post](https://pganalyze.com/blog/how-postgres-chooses-index).

Note that the Index Advisor planning process utilizes your production
statistics, and therefore costs are based on the actual measured size of tables
and indexes, where possible.

When you see the Scans list on the Index Advisor opportunity page, each scan
expression includes information about the current cost, estimated scans per
minute, and weighted cost improvement if you were to apply the recommendation.
Cost improvements are expressed as a multiplier, for example: 1.2x faster
compared to the old estimated cost. The table is sorted by cost improvement.


## Weighted cost improvement

During the Index Advisor recommendation process, different variants of indexes
are tried that overall produce the lowest cost (and thus the lowest estimated
I/O). When a recommendation is found, the Index Advisor will show an expected
weighted cost improvement for the recommendation. The weighted cost improvement
is a weighted average across the individual cost improvements for each scan that
can benefit from the query, weighted by the frequency of the scan (based on the
queries that contain the scan). More frequent queries will have a bigger impact
on the overall weighted cost improvement number.

## Index Write Overhead

To convey the cost of maintaining existing and proposed indexes, we define the
Index Write Overhead metric as an estimate of the I/O impact of maintaining that
index. This can guide you to consolidate indexes and drop little-used indexes to
have more I/O headroom. This estimate will be available for both existing
indexes, as well as new suggested indexes.


### Index Writes in Postgres

To understand this metric, it's important to understand the basics of how indexes
are maintained in Postgres. When data is written to a table, indexes referencing
that data may also need to be updated. Generally, indexes must be updated for
inserts and updates, and when the table is VACUUMed (deletes are "free", but do
incur a cost during VACUUM).

The size of an index entry for a particular indexed value will depend on the
the index type and the columns being indexed (plus
any columns added with [INCLUDE](https://www.postgresql.org/docs/current/sql-createindex.html)).

When considering index write overhead in aggregate, index storage parameters
like `fillfactor`, [b-tree
deduplication](https://www.postgresql.org/docs/current/btree-implementation.html#BTREE-DEDUPLICATION)
(in Postgres 13+), and bottom-up index deletion (in Postgres 14+) can also
affect the frequency of index writes. Similarly, partial indexes will only
need to be updated for changes to rows matching their index predicate. Finally,
indexes do not need to be touched for [HOT updates](https://git.postgresql.org/gitweb/?p=postgresql.git;a=blob;f=src/backend/access/heap/README.HOT;hb=HEAD).


### Modeling Index Write Overhead

To give a sense of the impact of writes, pganalyze models a simplified version
of the above mechanism. The metric is defined as "bytes written to the index
per byte written to the table": for every byte written to the table, this many
bytes must be written to the index.

The actual calculation is straightforward, though it involves some Postgres
internals. The basic formula for overhead is:

```
write overhead = index entry size  / row size * partial index selectivity
```

The index entry size is an 8-byte header plus the average width (based on
[column stats](/docs/install/troubleshooting/column_stats_helper) if available;
otherwise using a generic estimate based on data type) of all indexed columns.

The row size is a 23-byte header, plus a 4-byte item pointer, plus the average
width (again, based on column stats if available) of all the columns in the
table.

The partial index selectivity is 1 for normal indexes (without a `WHERE`
clause), and generically estimated at 10% for partial indexes.

### Index Write Overhead Example

Let's walk through a simple example. Let's say you have a schema like

```sql
CREATE TABLE users (
  id bigserial primary key,
  email text,
  password_hash bytea,
  organization_id bigint,
  team_id bigint
);
CREATE INDEX ON users (organization_id);
```

To estimate the write overhead for the organization_id index, first we would
look at the table stats. Say the column stats show the following average column
widths:

```
             id =  8
          email = 20
  password_hash = 60
organization_id =  8
        team_id =  8
```

With this, we can calculate the index entry size as

```
8 bytes (header) + 8 bytes (organization_id) + 8 bytes (team_id) = 24 bytes
```

Similarly, the row size is

```
23 bytes (header) + 4 bytes (item pointer) + 8 + 20 + 60 + 8 + 8 (all columns) = 131 bytes
```

Since this is not a partial index, the selectivity is 1.

The total index write overhead for this index is

```
24 / 131 * 1 ≈ 0.183
```

For every byte written to the table, approximately 0.183 bytes will be written
to update this index.


### Reasoning About Write Overhead

Unfortunately, there's no one right value for write overhead. Whether the number
from the example above is "good" or "bad" depends on several factors:

 - how much I/O headroom does the database have?
 - how frequently is this table written to? (exlcuding HOT updates which do not need to touch indexes)
 - how important is the latency for queries relying on this index (and how slow would they be without it)?

In general, it's useful to think of write overhead as a form of backpressure
against the temptation to just index everything. It's a way to convey that
indexing has a cost beyond just the disk space used, and to approximate that
cost as you compare different indexes.


### Limitations

Though Index Write Overhead is a useful metric, it's important to undertand it's
based on heuristics and simplifications. Here are the issues to be aware of:

 - currently only supported for b-tree indexes
 - uses a coarse heuristic for expression indexes (always assumes 8 bytes for expression result width)
 - INCLUDEd columns are not accounted for
 - partial index selectivity is always estimated at a generic 10%
 - NULL index entries are not sized correctly
 - memory alignment issues are ignored
 - compression of values is ignored
 - non-leaf pages in b-tree indexes are ignored (this is a reasonable approximation for even moderately large indexes)
